"""
Best Buy TV Detail Page Crawler
ìˆ˜ì§‘ í…Œì´ë¸”: bestbuy_tv_main_crawl, bby_tv_trend_crawl, bby_tv_promotion_crawl
ì €ì¥ í…Œì´ë¸”: bby_tv_detail_crawled
"""
import time
import random
import psycopg2
from datetime import datetime
import pytz
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from lxml import html

# DB ì„¤ì •
DB_CONFIG = {
    'host': 'samsung-dx-crawl.csnixzmkuppn.ap-northeast-2.rds.amazonaws.com',
    'port': 5432,
    'database': 'postgres',
    'user': 'postgres',
    'password': 'admin2025!'
}

class BestBuyDetailCrawler:
    def __init__(self):
        self.driver = None
        self.db_conn = None
        self.korea_tz = pytz.timezone('Asia/Seoul')
        self.batch_id = datetime.now(self.korea_tz).strftime('%Y%m%d_%H%M%S')
        self.order = 0

    def connect_db(self):
        """DB ì—°ê²°"""
        try:
            self.db_conn = psycopg2.connect(**DB_CONFIG)
            self.db_conn.autocommit = True
            print("[OK] Database connected")
            return True
        except Exception as e:
            print(f"[ERROR] Database connection failed: {e}")
            return False

    def setup_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì„¤ì •"""
        try:
            print("ğŸ”§ Chrome ë“œë¼ì´ë²„ ì„¤ì • ì¤‘...")
            self.driver = uc.Chrome()
            self.driver.maximize_window()
            print("âœ… ë“œë¼ì´ë²„ ì„¤ì • ì™„ë£Œ")
            return True
        except Exception as e:
            print(f"âŒ ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False

    def get_recent_urls(self):
        """ìµœì‹  batch_idì˜ product URLs ê°€ì ¸ì˜¤ê¸°"""
        try:
            cursor = self.db_conn.cursor()
            urls = []

            # bestbuy_tv_main_crawlì—ì„œ ìµœì‹  batch_id ê°€ì ¸ì˜¤ê¸°
            cursor.execute("""
                SELECT batch_id
                FROM bestbuy_tv_main_crawl
                WHERE batch_id IS NOT NULL
                ORDER BY batch_id DESC
                LIMIT 1
            """)
            main_batch_result = cursor.fetchone()
            main_batch_id = main_batch_result[0] if main_batch_result else None

            # bby_tv_Trend_crawlì—ì„œ ìµœì‹  batch_id ê°€ì ¸ì˜¤ê¸°
            cursor.execute("""
                SELECT batch_id
                FROM bby_tv_Trend_crawl
                WHERE batch_id IS NOT NULL
                ORDER BY batch_id DESC
                LIMIT 1
            """)
            trend_batch_result = cursor.fetchone()
            trend_batch_id = trend_batch_result[0] if trend_batch_result else None

            # bby_tv_promotion_crawlì—ì„œ ìµœì‹  batch_id ê°€ì ¸ì˜¤ê¸°
            cursor.execute("""
                SELECT batch_id
                FROM bby_tv_promotion_crawl
                WHERE batch_id IS NOT NULL
                ORDER BY batch_id DESC
                LIMIT 1
            """)
            promo_batch_result = cursor.fetchone()
            promo_batch_id = promo_batch_result[0] if promo_batch_result else None

            print(f"[INFO] Latest batch_id - Main: {main_batch_id}, Trend: {trend_batch_id}, Promotion: {promo_batch_id}")

            # bestbuy_tv_main_crawlì—ì„œ í•´ë‹¹ batchì˜ URLs ê°€ì ¸ì˜¤ê¸°
            if main_batch_id:
                cursor.execute("""
                    SELECT DISTINCT product_url
                    FROM bestbuy_tv_main_crawl
                    WHERE batch_id = %s
                    AND product_url IS NOT NULL
                """, (main_batch_id,))
                main_urls = cursor.fetchall()
                urls.extend([('main', url[0]) for url in main_urls])
                print(f"[OK] Main URLs (batch {main_batch_id}): {len(main_urls)}ê°œ")

            # bby_tv_Trend_crawlì—ì„œ í•´ë‹¹ batchì˜ URLs ê°€ì ¸ì˜¤ê¸°
            if trend_batch_id:
                cursor.execute("""
                    SELECT DISTINCT product_url
                    FROM bby_tv_Trend_crawl
                    WHERE batch_id = %s
                    AND product_url IS NOT NULL
                """, (trend_batch_id,))
                trend_urls = cursor.fetchall()
                urls.extend([('Trend', url[0]) for url in trend_urls])
                print(f"[OK] Trend URLs (batch {trend_batch_id}): {len(trend_urls)}ê°œ")

            # bby_tv_promotion_crawlì—ì„œ í•´ë‹¹ batchì˜ URLs ê°€ì ¸ì˜¤ê¸°
            if promo_batch_id:
                cursor.execute("""
                    SELECT DISTINCT product_url
                    FROM bby_tv_promotion_crawl
                    WHERE batch_id = %s
                    AND product_url IS NOT NULL
                """, (promo_batch_id,))
                promo_urls = cursor.fetchall()
                urls.extend([('promotion', url[0]) for url in promo_urls])
                print(f"[OK] Promotion URLs (batch {promo_batch_id}): {len(promo_urls)}ê°œ")

            cursor.close()
            print(f"[OK] ì´ {len(urls)}ê°œ URLs ë¡œë“œ ì™„ë£Œ")
            return urls

        except Exception as e:
            print(f"[ERROR] Failed to load URLs: {e}")
            return []

    def extract_retailer_sku_name(self, tree):
        """Retailer_SKU_Name ì¶”ì¶œ"""
        try:
            xpaths = [
                '//h1[contains(@class, "h4")]',
                '//div[@class="sku-title"]//h1'
            ]
            for xpath in xpaths:
                elem = tree.xpath(xpath)
                if elem:
                    return elem[0].text_content().strip()
            return None
        except Exception as e:
            print(f"  [ERROR] Retailer_SKU_Name ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def click_specifications(self):
        """Specification ë²„íŠ¼ í´ë¦­"""
        try:
            print("  [INFO] Specification ë²„íŠ¼ í´ë¦­...")
            # XPathë¥¼ ì‚¬ìš©í•œ ì—¬ëŸ¬ ì‹œë„
            xpaths = [
                "//button[@class='c-button-unstyled specs-accordion font-weight-medium w-full flex justify-content-between align-items-center CiN3vihE2Ub2POwD']",
                "//button[.//h3[text()='Specifications']]",
                "//button[contains(@class, 'specs-accordion')]"
            ]

            for xpath in xpaths:
                try:
                    spec_button = self.driver.find_element(By.XPATH, xpath)
                    self.driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", spec_button)
                    time.sleep(1)
                    spec_button.click()
                    print("  [OK] Specification í´ë¦­ ì„±ê³µ")
                    time.sleep(3)  # ë‹¤ì´ì–¼ë¡œê·¸ ë¡œë”© ëŒ€ê¸°
                    return True
                except:
                    continue

            print("  [WARNING] Specification ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False

        except Exception as e:
            print(f"  [ERROR] Specification í´ë¦­ ì‹¤íŒ¨: {e}")
            return False

    def extract_samsung_sku_name(self, tree):
        """Samsung_SKU_Name (Model Number) ì¶”ì¶œ"""
        try:
            # General ì„¹ì…˜ì—ì„œ Model Number ì°¾ê¸°
            xpaths = [
                '//li[.//h4[text()="General"]]//div[.//div[text()="Model Number"]]//div[@class="grow basis-none pl-300"]',
                '//div[contains(text(), "Model Number")]/following-sibling::div[@class="grow basis-none pl-300"]'
            ]
            for xpath in xpaths:
                elem = tree.xpath(xpath)
                if elem:
                    return elem[0].text_content().strip()
            return None
        except Exception as e:
            print(f"  [ERROR] Samsung_SKU_Name ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def extract_electricity_use(self, tree):
        """Estimated_Annual_Electricity_Use ì¶”ì¶œ"""
        try:
            # Power ì„¹ì…˜ì—ì„œ Estimated Annual Electricity Use ì°¾ê¸°
            xpaths = [
                '//li[.//h4[text()="Power"]]//div[.//div[contains(text(), "Estimated Annual Electricity Use")]]//div[@class="grow basis-none pl-300"]',
                '//div[contains(text(), "Estimated Annual Electricity Use")]/following-sibling::div[@class="grow basis-none pl-300"]'
            ]
            for xpath in xpaths:
                elem = tree.xpath(xpath)
                if elem:
                    return elem[0].text_content().strip()
            return None
        except Exception as e:
            print(f"  [ERROR] Estimated_Annual_Electricity_Use ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def close_specifications_dialog(self):
        """Specification ë‹¤ì´ì–¼ë¡œê·¸ ë‹«ê¸°"""
        try:
            print("  [INFO] Specification ë‹¤ì´ì–¼ë¡œê·¸ ë‹«ê¸°...")
            xpaths = [
                '//button[@data-testid="brix-sheet-closeButton"]',
                '//button[@aria-label="Close Sheet"]',
                '//div[@class="relative"]//button'
            ]

            for xpath in xpaths:
                try:
                    close_button = self.driver.find_element(By.XPATH, xpath)
                    close_button.click()
                    print("  [OK] ë‹¤ì´ì–¼ë¡œê·¸ ë‹«ê¸° ì„±ê³µ")
                    time.sleep(2)
                    return True
                except:
                    continue

            print("  [WARNING] ë‹¤ì´ì–¼ë¡œê·¸ ë‹«ê¸° ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False

        except Exception as e:
            print(f"  [ERROR] ë‹¤ì´ì–¼ë¡œê·¸ ë‹«ê¸° ì‹¤íŒ¨: {e}")
            return False

    def extract_similar_products(self, tree):
        """Compare similar products ë°ì´í„° ì¶”ì¶œ"""
        try:
            similar_names = []
            pros_list = []
            cons_list = []

            # Retailer_SKU_Name_similar ì¶”ì¶œ
            name_elements = tree.xpath('//span[@class="clamp" and starts-with(@id, "compare-title-")]')
            for elem in name_elements[:4]:  # ìµœëŒ€ 4ê°œ
                similar_names.append(elem.text_content().strip())

            # Pros ì¶”ì¶œ
            pros_elements = tree.xpath('//tr[@class="flex"]//td[.//svg[@aria-label="Advantage Icon"]]//span[@class="text-3 min-w-0 flex flex-wrap"]')
            for elem in pros_elements[:4]:  # ìµœëŒ€ 4ê°œ
                pros_list.append(elem.text_content().strip())

            # Cons ì¶”ì¶œ
            cons_elements = tree.xpath('//tr[@class="flex"]//td[.//svg[@aria-label="Disadvantage Icon"]]//span[@class="text-3 min-w-0 flex flex-wrap"]')
            for elem in cons_elements[:4]:  # ìµœëŒ€ 4ê°œ
                text = elem.text_content().strip()
                if text and text != 'â€”':
                    cons_list.append(text)
                else:
                    cons_list.append(None)

            # ë¶€ì¡±í•œ ê²½ìš° Noneìœ¼ë¡œ ì±„ìš°ê¸°
            while len(similar_names) < 4:
                similar_names.append(None)
            while len(pros_list) < 4:
                pros_list.append(None)
            while len(cons_list) < 4:
                cons_list.append(None)

            return similar_names[:4], pros_list[:4], cons_list[:4]

        except Exception as e:
            print(f"  [ERROR] Similar products ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return [None]*4, [None]*4, [None]*4

    def extract_star_ratings(self, tree):
        """Count_of_Star_Ratings ì¶”ì¶œ"""
        try:
            ratings = {}
            # ë³„ì ë³„ ê°œìˆ˜ ì¶”ì¶œ
            for star in range(1, 6):
                xpath = f'//a[contains(@href, "rating={star}")]//span[@class="uq0khJy2NYfr1ydM text-left v-text-tech-black"]'
                elem = tree.xpath(xpath)
                if elem:
                    count = elem[0].text_content().strip()
                    ratings[f"{star}stars"] = count
                else:
                    ratings[f"{star}stars"] = "0"

            # í˜•ì‹: "5stars:776, 4stars:156, 3stars:29, 2stars:14, 1star:30"
            rating_str = ", ".join([f"{k}:{v}" for k, v in ratings.items()])
            return rating_str if rating_str else None

        except Exception as e:
            print(f"  [ERROR] Star ratings ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def extract_top_mentions(self, tree):
        """Top_Mentions ì¶”ì¶œ"""
        try:
            mentions = []
            mention_elements = tree.xpath('//li[contains(@class, "inline-block")]//a[contains(@class, "pPqRKazD1ugrkdAf")]')
            for elem in mention_elements[:10]:  # ìµœëŒ€ 10ê°œ
                text = elem.text_content().strip()
                mentions.append(text)

            return ", ".join(mentions) if mentions else None

        except Exception as e:
            print(f"  [ERROR] Top mentions ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def click_see_all_reviews(self):
        """See All Customer Reviews ë²„íŠ¼ í´ë¦­"""
        try:
            print("  [INFO] See All Customer Reviews ë²„íŠ¼ í´ë¦­...")
            xpaths = [
                '//button[contains(., "See All Customer Reviews")]',
                '//button[@class="relative border-xs border-solid rounded-lg justify-center items-center self-start flex flex-col cursor-pointer px-300 py-100 border-comp-outline-primary-emphasis bg-comp-surface-primary-emphasis mr-200 Op9coqeII1kYHR9Q"]'
            ]

            for xpath in xpaths:
                try:
                    button = self.driver.find_element(By.XPATH, xpath)
                    self.driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", button)
                    time.sleep(1)
                    button.click()
                    print("  [OK] See All Customer Reviews í´ë¦­ ì„±ê³µ")
                    time.sleep(3)
                    return True
                except:
                    continue

            print("  [WARNING] See All Customer Reviews ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False

        except Exception as e:
            print(f"  [ERROR] See All Customer Reviews í´ë¦­ ì‹¤íŒ¨: {e}")
            return False

    def extract_reviews(self):
        """ë¦¬ë·° 20ê°œ ìˆ˜ì§‘ (í˜ì´ì§€ë„¤ì´ì…˜ í¬í•¨)"""
        try:
            reviews = []
            collected = 0
            page = 1

            while collected < 20:
                # í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
                page_source = self.driver.page_source
                tree = html.fromstring(page_source)

                # ë¦¬ë·° ì¶”ì¶œ
                review_elements = tree.xpath('//li[@class="review-item"]//div[@class="ugc-review-body"]//p[@class="pre-white-space"]')

                for elem in review_elements:
                    if collected >= 20:
                        break
                    review_text = elem.text_content().strip()
                    if review_text:
                        reviews.append(review_text)
                        collected += 1
                        print(f"    [ë¦¬ë·° {collected}/20] {review_text[:50]}...")

                # 20ê°œ ìˆ˜ì§‘ ì™„ë£Œí•˜ë©´ ì¢…ë£Œ
                if collected >= 20:
                    break

                # ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ ì°¾ê¸°
                try:
                    next_button = self.driver.find_element(By.XPATH, '//li[contains(@class, "page next")]//a')
                    print(f"  [INFO] ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™ ì¤‘... (Page {page + 1})")
                    self.driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", next_button)
                    time.sleep(1)
                    next_button.click()
                    time.sleep(3)
                    page += 1
                except:
                    print("  [INFO] ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ì´ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ì§‘ ì¢…ë£Œ.")
                    break

            # ë¦¬ë·°ë¥¼ êµ¬ë¶„ìë¡œ ì—°ê²°
            return " | ".join(reviews) if reviews else None

        except Exception as e:
            print(f"  [ERROR] ë¦¬ë·° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return None

    def extract_recommendation_intent(self, tree):
        """Recommendation_Intent ì¶”ì¶œ"""
        try:
            xpaths = [
                '//div[contains(@class, "v-text-dark-gray") and contains(., "would recommend")]',
                '//div[.//svg[contains(@class, "mr-50")]]//span[@class="font-weight-bold"]/parent::*/text()'
            ]

            # ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            elem = tree.xpath('//div[contains(@class, "v-text-dark-gray") and contains(., "would recommend")]')
            if elem:
                text = elem[0].text_content().strip()
                # "93% would recommend to a friend" í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œ
                text = ' '.join(text.split())  # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
                return text

            return None

        except Exception as e:
            print(f"  [ERROR] Recommendation intent ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def scrape_detail_page(self, page_type, product_url):
        """ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§"""
        try:
            self.order += 1
            print(f"\n[{self.order}] [{page_type}] {product_url[:80]}...")

            # í˜ì´ì§€ ì ‘ì†
            self.driver.get(product_url)
            time.sleep(random.uniform(5, 8))

            # í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
            page_source = self.driver.page_source
            tree = html.fromstring(page_source)

            # 1. Retailer_SKU_Name ì¶”ì¶œ
            retailer_sku_name = self.extract_retailer_sku_name(tree)
            print(f"  [âœ“] Retailer_SKU_Name: {retailer_sku_name}")

            # 2. Specification ë²„íŠ¼ í´ë¦­
            samsung_sku_name = None
            electricity_use = None

            if self.click_specifications():
                time.sleep(2)
                # ë‹¤ì´ì–¼ë¡œê·¸ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
                dialog_source = self.driver.page_source
                dialog_tree = html.fromstring(dialog_source)

                # 3. Samsung_SKU_Name ì¶”ì¶œ
                samsung_sku_name = self.extract_samsung_sku_name(dialog_tree)
                print(f"  [âœ“] Samsung_SKU_Name: {samsung_sku_name}")

                # 4. Estimated_Annual_Electricity_Use ì¶”ì¶œ
                electricity_use = self.extract_electricity_use(dialog_tree)
                print(f"  [âœ“] Estimated_Annual_Electricity_Use: {electricity_use}")

                # 5. ë‹¤ì´ì–¼ë¡œê·¸ ë‹«ê¸°
                self.close_specifications_dialog()

            # í˜ì´ì§€ ë‹¤ì‹œ ë¡œë“œ
            page_source = self.driver.page_source
            tree = html.fromstring(page_source)

            # 6. Similar products ì¶”ì¶œ
            similar_names, pros_list, cons_list = self.extract_similar_products(tree)
            print(f"  [âœ“] Similar products: {len([x for x in similar_names if x])}ê°œ")

            # 7. Star ratings ì¶”ì¶œ
            star_ratings = self.extract_star_ratings(tree)
            print(f"  [âœ“] Star_Ratings: {star_ratings}")

            # 8. Top mentions ì¶”ì¶œ
            top_mentions = self.extract_top_mentions(tree)
            print(f"  [âœ“] Top_Mentions: {top_mentions}")

            # 9. See All Customer Reviews í´ë¦­ ë° ë¦¬ë·° ìˆ˜ì§‘
            detailed_reviews = None
            if self.click_see_all_reviews():
                detailed_reviews = self.extract_reviews()
                print(f"  [âœ“] Detailed_Reviews: {len(detailed_reviews) if detailed_reviews else 0} chars")

            # 10. Recommendation intent ì¶”ì¶œ
            page_source = self.driver.page_source
            tree = html.fromstring(page_source)
            recommendation_intent = self.extract_recommendation_intent(tree)
            print(f"  [âœ“] Recommendation_Intent: {recommendation_intent}")

            # DB ì €ì¥
            self.save_to_db(
                page_type=page_type,
                order=self.order,
                retailer_sku_name=retailer_sku_name,
                samsung_sku_name=samsung_sku_name,
                electricity_use=electricity_use,
                similar_names=similar_names,
                pros_list=pros_list,
                cons_list=cons_list,
                star_ratings=star_ratings,
                top_mentions=top_mentions,
                detailed_reviews=detailed_reviews,
                recommendation_intent=recommendation_intent,
                product_url=product_url
            )

            return True

        except Exception as e:
            print(f"  [ERROR] ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False

    def save_to_db(self, page_type, order, retailer_sku_name, samsung_sku_name,
                   electricity_use, similar_names, pros_list, cons_list,
                   star_ratings, top_mentions, detailed_reviews,
                   recommendation_intent, product_url):
        """DBì— ì €ì¥"""
        try:
            cursor = self.db_conn.cursor()

            # í…Œì´ë¸” ì¡´ì¬ í™•ì¸ ë° ìƒì„±
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS bby_tv_detail_crawled (
                    id SERIAL PRIMARY KEY,
                    batch_id VARCHAR(50),
                    page_type VARCHAR(50),
                    "order" INTEGER,
                    Retailer_SKU_Name TEXT,
                    Samsung_SKU_Name TEXT,
                    Estimated_Annual_Electricity_Use TEXT,
                    Retailer_SKU_Name_similar TEXT,
                    Pros TEXT,
                    Cons TEXT,
                    Count_of_Star_Ratings TEXT,
                    Top_Mentions TEXT,
                    Detailed_Review_Content TEXT,
                    Recommendation_Intent TEXT,
                    product_url TEXT,
                    crawl_datetime TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # ë°ì´í„° ì‚½ì…
            insert_query = """
                INSERT INTO bby_tv_detail_crawled
                (batch_id, page_type, "order", Retailer_SKU_Name, Samsung_SKU_Name,
                 Estimated_Annual_Electricity_Use, Retailer_SKU_Name_similar,
                 Pros, Cons, Count_of_Star_Ratings, Top_Mentions,
                 Detailed_Review_Content, Recommendation_Intent, product_url)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            """

            # Similar productsë¥¼ êµ¬ë¶„ìë¡œ ì—°ê²°
            similar_names_str = " | ".join([x for x in similar_names if x]) if any(similar_names) else None
            pros_str = " | ".join([x for x in pros_list if x]) if any(pros_list) else None
            cons_str = " | ".join([x for x in cons_list if x]) if any(cons_list) else None

            cursor.execute(insert_query, (
                self.batch_id,
                page_type,
                order,
                retailer_sku_name,
                samsung_sku_name,
                electricity_use,
                similar_names_str,
                pros_str,
                cons_str,
                star_ratings,
                top_mentions,
                detailed_reviews,
                recommendation_intent,
                product_url
            ))

            cursor.close()
            print(f"  [âœ“] DB ì €ì¥ ì™„ë£Œ")
            return True

        except Exception as e:
            print(f"  [ERROR] DB ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False

    def run(self):
        """ë©”ì¸ ì‹¤í–‰"""
        try:
            print("="*80)
            print(f"Best Buy TV Detail Page Crawler (Batch ID: {self.batch_id})")
            print("="*80)

            # DB ì—°ê²°
            if not self.connect_db():
                return

            # URLs ê°€ì ¸ì˜¤ê¸°
            urls = self.get_recent_urls()
            if not urls:
                print("[ERROR] No URLs found")
                return

            # ë“œë¼ì´ë²„ ì„¤ì •
            if not self.setup_driver():
                return

            # ê° URL í¬ë¡¤ë§
            success_count = 0
            for page_type, url in urls:
                if self.scrape_detail_page(page_type, url):
                    success_count += 1

                # í˜ì´ì§€ ê°„ ë”œë ˆì´
                time.sleep(random.uniform(3, 5))

            print("\n" + "="*80)
            print(f"í¬ë¡¤ë§ ì™„ë£Œ! ì„±ê³µ: {success_count}/{len(urls)}ê°œ")
            print("="*80)

        except Exception as e:
            print(f"âŒ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()

        finally:
            if self.driver:
                self.driver.quit()
                print("\nğŸ”§ ë“œë¼ì´ë²„ ì¢…ë£Œ")
            if self.db_conn:
                self.db_conn.close()
                print("ğŸ”§ DB ì—°ê²° ì¢…ë£Œ")

def main():
    crawler = BestBuyDetailCrawler()
    crawler.run()

if __name__ == "__main__":
    main()
