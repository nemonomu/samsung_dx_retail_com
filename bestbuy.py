"""
BestBuy ê°€ê²© ì¶”ì¶œ ì‹œìŠ¤í…œ - DB ê¸°ë°˜ ë²„ì „
DBì—ì„œ URL ì½ì–´ì™€ì„œ í¬ë¡¤ë§ í›„ ê²°ê³¼ ì €ì¥
íŒŒì¼ëª… í˜•ì‹: {ìˆ˜ì§‘ì¼ì}{ìˆ˜ì§‘ì‹œê°„}_{êµ­ê°€ì½”ë“œ}_{ì‡¼í•‘ëª°}.csv
"""
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
import pymysql
from sqlalchemy import create_engine
import paramiko
import time
import random
import re
from datetime import datetime
import pytz
import logging
import os
from io import StringIO

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

# Import database configuration
from config import DB_CONFIG

# íŒŒì¼ì„œë²„ ì„¤ì •
FILE_SERVER_CONFIG = {
    'host': '3.36.101.24',
    'port': 22,
    'username': 'ftpuser',
    'password': 'samsung0701!',
    'upload_path': '/home/ftpuser/uploads'
}

class BestBuyScraper:
    def __init__(self):
        self.driver = None
        self.db_engine = None
        self.sftp_client = None
        self.session_initialized = False
        self.korea_tz = pytz.timezone('Asia/Seoul')

        # DB ì—°ê²° ì„¤ì •
        self.setup_db_connection()
        
        # DBì—ì„œ XPath ë¡œë“œ
        self.load_xpaths_from_db()
        
    def setup_db_connection(self):
        """DB ì—°ê²° ì„¤ì •"""
        try:
            # SQLAlchemy ì—”ì§„ ìƒì„±
            connection_string = (
                f"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@"
                f"{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}"
            )
            self.db_engine = create_engine(connection_string)
            logger.info("âœ… DB ì—°ê²° ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ DB ì—°ê²° ì‹¤íŒ¨: {e}")
            self.db_engine = None
    
    def load_xpaths_from_db(self):
        """DBì—ì„œ BestBuyìš© ì„ íƒì ë¡œë“œ"""
        try:
            query = """
            SELECT element_type, selector_value, priority
            FROM mall_selectors
            WHERE mall_name = 'bestbuy'
              AND country_code = 'usa'
              AND is_active = TRUE
            ORDER BY element_type, priority DESC
            """

            df = pd.read_sql(query, self.db_engine)

            # element_typeë³„ë¡œ ê·¸ë£¹í™” (price, imageurl, title ì œì™¸)
            self.XPATHS = {}
            for element_type in df['element_type'].unique():
                if element_type not in ['price', 'imageurl', 'title']:  # price, imageurl, title ì„ íƒìëŠ” DBì—ì„œ ë¡œë“œí•˜ì§€ ì•ŠìŒ
                    type_selectors = df[df['element_type'] == element_type]['selector_value'].tolist()
                    self.XPATHS[element_type] = type_selectors

            logger.info(f"âœ… DBì—ì„œ ì„ íƒì ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ")

            # price ì„ íƒìëŠ” í•­ìƒ í•˜ë“œì½”ë”©ëœ ê°’ ì‚¬ìš© (DB ë¬´ì‹œ)
            self.XPATHS['price'] = [
                '/html/body/div[5]/div[4]/div[1]/div/div[4]/div/div/div[1]/div/div[1]/div[1]/div[1]/div/div/div/div[1]/span',
                '/html/body/div[5]/div[4]/div[1]/div/div[5]/div/div/div[1]/div/div[1]/div[1]/div[1]/div/div/div/div[1]/span',
                '/html/body/div[5]/div[4]/div[2]/div/div[3]/div/div/div[1]/div/div[1]/div[2]/div[1]/div/div/div/div[1]',
                '/html/body/div[5]/div[4]/div[2]/div/div[3]/div/div/div[1]/div/div[1]/div[2]/div[1]/div/div/div/div[1]/span',
                '/html/body/div[5]/div[4]/div[2]/div/div[3]/div/div/div[1]/div/div[1]/div[1]/div[1]/div/div/div/div[1]/span',
                '/html/body/div[5]/div[4]/div[2]/div/div[4]/div/div/div[1]/div/div[1]/div[1]/div[1]/div/div/div/div[1]/span',
                '/html/body/div[5]/div[4]/div[2]/div/div[5]/div/div/div[1]/div/div[1]/div[1]/div[1]/div/div/div/div[1]/span',
                '/html/body/div[5]/div[4]/div[2]/div/div[4]/div/div/div/div/div[2]/div[1]/div[1]',
                '/html/body/div[5]/div[4]/div[2]/div/div[5]/div/div/div/div/div[2]/div[1]/div[1]',
                '/html/body/div[5]/div[4]/div[2]/div/div[4]/div/div/div[1]/div/div[1]/div[2]/div[1]/div/div/div/div[1]/span'
            ]

            # title ì„ íƒìëŠ” í•­ìƒ í•˜ë“œì½”ë”©ëœ ê°’ ì‚¬ìš© (DB ë¬´ì‹œ)
            self.XPATHS['title'] = [
                '/html/body/div[5]/div[4]/div[2]/div/h1',
                '//h1[@class="sku-title"]',
                '//div[@class="sku-title"]//h1'
            ]

            # imageurl ì„ íƒìëŠ” í•­ìƒ í•˜ë“œì½”ë”©ëœ ê°’ ì‚¬ìš© (DB ë¬´ì‹œ)
            self.XPATHS['imageurl'] = [
                '/html/body/div[5]/div[4]/div[1]/div/div[2]/div[2]/div/div[2]/div/button[1]/img',
                '//img[@class="primary-image"]',
                '//div[@class="media-gallery"]//img'
            ]

            # ê¸°ë³¸ê°’ ì„¤ì • (DBì— ì—†ëŠ” ê²½ìš° - price, imageurl, title ì œì™¸)
            if not self.XPATHS.get('stock_flag'):
                logger.warning("âš ï¸ DBì— ì„ íƒìê°€ ì—†ì–´ ê¸°ë³¸ê°’ ì‚¬ìš©")
                self.XPATHS.update({
                    'imageurl_fallback': [
                        '/html/body/div[5]/div[3]/div[1]/div/div[1]/img'
                    ],
                    'stock_flag': [
                        'Out of Stock', 'Sold Out', 'Currently unavailable',
                        'Coming Soon', 'Temporarily out of stock'
                    ],
                    'country_select': [
                        '/html/body/div[2]/div/div/div/div[1]/div[2]/a[2]'
                    ]
                })

                # fallback ì´ë¯¸ì§€ ì„ íƒì ì¶”ê°€
                if 'imageurl_fallback' not in self.XPATHS:
                    self.XPATHS['imageurl_fallback'] = ['/html/body/div[5]/div[3]/div[1]/div/div[1]/img']
                
        except Exception as e:
            logger.error(f"ì„ íƒì ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ê°’ ì‚¬ìš©
            self.XPATHS = {
                'price': [
                    '/html/body/div[5]/div[4]/div[2]/div/div[3]/div/div/div[1]/div/div[1]/div[2]/div[1]/div/div/div/div[1]',
                    '/html/body/div[5]/div[4]/div[2]/div/div[3]/div/div/div[1]/div/div[1]/div[2]/div[1]/div/div/div/div[1]/span',
                    '/html/body/div[5]/div[4]/div[2]/div/div[3]/div/div/div[1]/div/div[1]/div[1]/div[1]/div/div/div/div[1]/span',
                    '/html/body/div[5]/div[4]/div[2]/div/div[4]/div/div/div[1]/div/div[1]/div[1]/div[1]/div/div/div/div[1]/span'
                ],
                'title': [],
                'imageurl': [],
                'imageurl_fallback': ['/html/body/div[5]/div[3]/div[1]/div/div[1]/img'],
                'stock_flag': [],
                'country_select': []
            }
    
    def get_crawl_targets(self, limit=None, include_failed=False):
        """DBì—ì„œ í¬ë¡¤ë§ ëŒ€ìƒ URL ëª©ë¡ ì¡°íšŒ"""
        try:
            if include_failed:
                # ìµœê·¼ ì‹¤íŒ¨í•œ URLë„ í¬í•¨ (24ì‹œê°„ ì´ë‚´ ì‹¤íŒ¨ 3íšŒ ë¯¸ë§Œ)
                query = """
                WITH failed_counts AS (
                    SELECT url, COUNT(*) as fail_count
                    FROM amazon_crawl_logs
                    WHERE retailprice IS NULL  
                      AND crawl_datetime >= DATE_SUB(NOW(), INTERVAL 24 HOUR)
                      AND country_code = 'usa'
                    GROUP BY url
                )
                SELECT DISTINCT t.*
                FROM samsung_price_tracking_list t
                LEFT JOIN failed_counts f ON t.url = f.url
                WHERE t.country = 'usa' 
                  AND t.mall_name = 'bestbuy'
                  AND t.is_active = TRUE
                  AND (f.fail_count IS NULL OR f.fail_count < 3)
                ORDER BY COALESCE(f.fail_count, 0) DESC
                """
            else:
                query = """
                SELECT *
                FROM samsung_price_tracking_list
                WHERE country = 'usa' 
                  AND mall_name = 'bestbuy'
                  AND is_active = TRUE
                """
                
            if limit:
                query += f" LIMIT {limit}"
            
            df = pd.read_sql(query, self.db_engine)
            logger.info(f"âœ… í¬ë¡¤ë§ ëŒ€ìƒ {len(df)}ê°œ ì¡°íšŒ ì™„ë£Œ")
            return df.to_dict('records')
            
        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ëŒ€ìƒ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def setup_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì„¤ì •"""
        logger.info("ğŸ”§ Chrome ë“œë¼ì´ë²„ ì„¤ì • ì¤‘...")
        
        try:
            options = uc.ChromeOptions()
            # ìŠ¤í…”ìŠ¤ ëª¨ë“œ ì„¤ì •
            options.add_argument('--disable-blink-features=AutomationControlled')
            
            self.driver = uc.Chrome(options=options)
            self.driver.maximize_window()
            
            # ì¶”ê°€ ìŠ¤í…”ìŠ¤ ì„¤ì •
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            self.driver.execute_script("Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3]})")
            
            logger.info("âœ… ë“œë¼ì´ë²„ ì„¤ì • ì™„ë£Œ")
            return True
        except Exception as e:
            logger.error(f"âŒ ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False
    
    def initialize_session(self):
        """BestBuy ì„¸ì…˜ ì´ˆê¸°í™” (êµ­ê°€ ì„ íƒ í¬í•¨)"""
        if self.session_initialized:
            return True
            
        try:
            logger.info("ğŸŒ BestBuy ì„¸ì…˜ ì´ˆê¸°í™” ì¤‘...")
            
            # BestBuy ë©”ì¸ í˜ì´ì§€ ì ‘ì†
            self.driver.get("https://www.bestbuy.com")
            time.sleep(4)
            
            # êµ­ê°€ ì„ íƒ íŒì—… ì²˜ë¦¬
            self.handle_country_popup()
            
            # ì„¸ì…˜ í™•ì¸
            title = self.driver.title
            if "Best Buy" in title:
                logger.info("âœ… BestBuy ì„¸ì…˜ ì´ˆê¸°í™” ì™„ë£Œ")
                self.session_initialized = True
                return True
            else:
                logger.warning("âš ï¸ ì„¸ì…˜ ì´ˆê¸°í™” ë¶€ë¶„ ì„±ê³µ")
                self.session_initialized = True
                return True
                
        except Exception as e:
            logger.error(f"âŒ ì„¸ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def handle_country_popup(self):
        """êµ­ê°€ ì„ íƒ íŒì—… ì²˜ë¦¬ (ì„¸ì…˜ë‹¹ 1íšŒ)"""
        try:
            logger.info("ğŸŒ êµ­ê°€ ì„ íƒ í™•ì¸ ì¤‘...")
            time.sleep(3)
            
            # DBì—ì„œ ê°€ì ¸ì˜¨ êµ­ê°€ ì„ íƒ ì…€ë ‰í„°
            country_selectors = self.XPATHS.get('country_select', [])
            
            # ê¸°ë³¸ ì…€ë ‰í„° ì¶”ê°€
            all_selectors = country_selectors + [
                "//a[contains(@class, 'us-link')]",
                "//button[contains(text(), 'United States')]"
            ]
            
            for selector in all_selectors:
                try:
                    if selector.startswith('//'):
                        element = self.driver.find_element(By.XPATH, selector)
                    else:
                        element = self.driver.find_element(By.CSS_SELECTOR, selector)
                    
                    if element.is_displayed():
                        element.click()
                        logger.info("ğŸ‡ºğŸ‡¸ ë¯¸êµ­ ì‚¬ì´íŠ¸ ì„ íƒ ì™„ë£Œ")
                        time.sleep(3)
                        return True
                except:
                    continue
            
            logger.info("êµ­ê°€ ì„ íƒ íŒì—… ì—†ìŒ (ì´ë¯¸ ì„¤ì •ë¨)")
            return True
            
        except Exception as e:
            logger.warning(f"êµ­ê°€ íŒì—… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")
            return True
    
    def wait_for_price_elements(self, max_wait=30):
        """ê°€ê²© ìš”ì†Œë“¤ì´ ì‹¤ì œë¡œ ë¡œë“œë  ë•Œê¹Œì§€ ìŠ¤ë§ˆíŠ¸ ëŒ€ê¸°"""
        
        # 1ë‹¨ê³„: ê¸°ë³¸ í˜ì´ì§€ êµ¬ì¡° ëŒ€ê¸°
        try:
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
        except:
            pass
        
        # 2ë‹¨ê³„: ê°€ê²© ê´€ë ¨ ì»¨í…Œì´ë„ˆë“¤ ëŒ€ê¸° (ì—¬ëŸ¬ í›„ë³´ ì¤‘ í•˜ë‚˜ë¼ë„ ë‚˜íƒ€ë‚˜ë©´ OK)
        price_containers = [
            "//div[contains(@class, 'pricing')]",
            "//div[contains(@class, 'price')]", 
            "//span[contains(@class, 'current-price')]",
            "//div[contains(@data-testid, 'pricing')]"
        ]
        
        for container in price_containers:
            try:
                WebDriverWait(self.driver, 5).until(
                    EC.presence_of_element_located((By.XPATH, container))
                )
                break
            except:
                continue
        
        # 3ë‹¨ê³„: ì‹¤ì œ ê°€ê²© ê°’ì´ ë“¤ì–´ì˜¬ ë•Œê¹Œì§€ ëŒ€ê¸°
        start_time = time.time()
        while time.time() - start_time < max_wait:
            for xpath in self.XPATHS.get('price', []):
                try:
                    element = self.driver.find_element(By.XPATH, xpath)
                    text = element.text.strip()
                    if text and ('$' in text or text.replace(',', '').replace('.', '').isdigit()):
                        logger.info(f"âœ… ê°€ê²© ìš”ì†Œ ë¡œë”© ì™„ë£Œ: {text}")
                        return True
                except:
                    continue
            time.sleep(1)  # 1ì´ˆë§ˆë‹¤ ì¬í™•ì¸
        
        logger.warning("âš ï¸ ê°€ê²© ìš”ì†Œ ë¡œë”© ì‹œê°„ ì´ˆê³¼")
        return False

    def wait_for_network_idle(self, idle_time=2):
        """ë„¤íŠ¸ì›Œí¬ ìš”ì²­ì´ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°"""
        try:
            # Performance API ì‚¬ìš©í•´ì„œ ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ì™„ë£Œ í™•ì¸
            script = """
            return window.performance.getEntriesByType('navigation')[0].loadEventEnd > 0
            """
            
            start_time = time.time()
            while time.time() - start_time < 20:  # ìµœëŒ€ 20ì´ˆ
                if self.driver.execute_script(script):
                    time.sleep(idle_time)  # ì¶”ê°€ ì•ˆì •í™” ì‹œê°„
                    return True
                time.sleep(0.5)
        except:
            pass
        return False

    def extract_product_info(self, url, row_data):
        """ì œí’ˆ ì •ë³´ ì¶”ì¶œ (ìŠ¤ë§ˆíŠ¸ ëŒ€ê¸° ë¡œì§ ì¶”ê°€)"""
        try:
            logger.info(f"ğŸ” í˜ì´ì§€ ì ‘ì†: {url}")
            
            # ì„¸ì…˜ ì´ˆê¸°í™” í™•ì¸
            if not self.session_initialized:
                self.initialize_session()
            
            self.driver.get(url)
            
            # ìŠ¤ë§ˆíŠ¸ ëŒ€ê¸° ì „ëµ ì ìš©
            logger.info("â³ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘...")
            
            # 1. ë„¤íŠ¸ì›Œí¬ ì™„ë£Œ ëŒ€ê¸°
            self.wait_for_network_idle()
            
            # 2. ê°€ê²© ìš”ì†Œë“¤ ë¡œë”© ëŒ€ê¸°
            if not self.wait_for_price_elements():
                logger.warning("ê°€ê²© ìš”ì†Œ ë¡œë”© ì‹¤íŒ¨, ê·¸ë˜ë„ ì¶”ì¶œ ì‹œë„")
            
            # 3. ì¶”ê°€ ì•ˆì •í™” ì‹œê°„
            time.sleep(random.uniform(2, 4))
            
            # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
            wait = WebDriverWait(self.driver, 20)
            
            # ì°¨ë‹¨ ê°ì§€
            title = self.driver.title
            blocked_patterns = ["Access Denied", "Blocked", "Robot", "Captcha", "Sorry", "Error"]
            for pattern in blocked_patterns:
                if pattern.lower() in title.lower():
                    logger.warning(f"âš ï¸ ì°¨ë‹¨ ê°ì§€: {pattern}")
                    raise Exception(f"Blocked: {pattern}")
            
            # í˜„ì¬ ì‹œê°„
            now_time = datetime.now(self.korea_tz)
            crawl_datetime_str = now_time.strftime('%Y-%m-%d %H:%M:%S')
            crawl_strdatetime = now_time.strftime('%Y%m%d%H%M%S') + f"{now_time.microsecond:06d}"[:4]
            
            # ê¸°ë³¸ ê²°ê³¼ êµ¬ì¡°
            result = {
                'retailerid': row_data.get('retailerid', ''),
                'country_code': row_data.get('country', 'usa'),
                'ships_from': 'usa',
                'channel': row_data.get('channel', 'Online'),
                'retailersku': row_data.get('retailersku', ''),
                'brand': row_data.get('brand', ''),
                'brand_eng': row_data.get('brand_eng', row_data.get('brand', '')),
                'form_factor': row_data.get('form_factor', ''),
                'segment_lv1': row_data.get('seg_lv1', ''),
                'segment_lv2': row_data.get('seg_lv2', ''),
                'segment_lv3': row_data.get('seg_lv3', ''),
                'capacity': row_data.get('capacity', ''),
                'item': row_data.get('item', ''),
                'retailprice': None,
                'sold_by': 'BestBuy',
                'imageurl': None,
                'producturl': url,
                'crawl_datetime': crawl_datetime_str,
                'crawl_strdatetime': crawl_strdatetime,
                'title': None,
                'vat': row_data.get('vat', 'x')
            }
            
            # ì¬ê³  ìƒíƒœ í™•ì¸
            page_source = self.driver.page_source
            stock_available = True
            
            for stock_flag in self.XPATHS.get('stock_flag', []):
                if stock_flag in page_source:
                    logger.info(f"ì¬ê³  ì—†ìŒ: {stock_flag}")
                    stock_available = False
                    break
            
            # ë””ë²„ê¹…: HTML ì €ì¥
            try:
                debug_filename = f"bestbuy_debug_{now_time.strftime('%Y%m%d_%H%M%S')}.html"
                with open(debug_filename, 'w', encoding='utf-8') as f:
                    f.write(self.driver.page_source)
                logger.info(f"ğŸ› ë””ë²„ê·¸ìš© HTML ì €ì¥: {debug_filename}")
            except Exception as e:
                logger.warning(f"HTML ì €ì¥ ì‹¤íŒ¨: {e}")

            # ê°€ê²© ì¶”ì¶œ
            price_found = False

            # 1ë‹¨ê³„: ê¸°ì¡´ ì„ íƒìë¡œ ì‹œë„
            logger.info("ğŸ’° ê¸°ì¡´ ê°€ê²© ì„ íƒìë¡œ ì‹œë„ ì¤‘...")
            for xpath in self.XPATHS.get('price', []):
                try:
                    price_element = self.driver.find_element(By.XPATH, xpath)
                    price_text = price_element.text.strip()
                    
                    logger.info(f"ğŸ” ì„ íƒì: {xpath}")
                    logger.info(f"ğŸ“ ì¶”ì¶œëœ í…ìŠ¤íŠ¸: '{price_text}'")
                    
                    if price_text:
                        # BestBuyëŠ” ì£¼ë¡œ $ ì‚¬ìš©
                        price_match = re.search(r'\$([\d,]+\.?\d*)', price_text)
                        if price_match:
                            price_number = price_match.group(1).replace(',', '')
                            result['retailprice'] = float(price_number)
                            logger.info(f"âœ… ê°€ê²© ì¶”ì¶œ ì„±ê³µ: ${result['retailprice']} (ì„ íƒì: {xpath})")
                            price_found = True
                            break
                        else:
                            logger.info(f"âŒ ê°€ê²© íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨: '{price_text}'")
                    else:
                        logger.info("âŒ ë¹ˆ í…ìŠ¤íŠ¸")
                except Exception as xe:
                    logger.info(f"âŒ ì„ íƒì ì‹¤í–‰ ì‹¤íŒ¨: {xe}")
                    continue
            
            # 2ë‹¨ê³„: CSS ì„ íƒìë¡œ ì¬ì‹œë„
            if not price_found:
                logger.info("ğŸ’° CSS ì„ íƒìë¡œ ì¬ì‹œë„ ì¤‘...")
                try:
                    css_xpath = "//span[@class='sr-only' and contains(text(), 'current price')]"
                    price_element = self.driver.find_element(By.XPATH, css_xpath)
                    price_text = price_element.text
                    if '$' in price_text:
                        price_match = re.search(r'\$([\d,]+\.?\d*)', price_text)
                        if price_match:
                            result['retailprice'] = float(price_match.group(1).replace(',', ''))
                            logger.info(f"âœ… ê°€ê²© ì¶”ì¶œ ì„±ê³µ (CSS): ${result['retailprice']}")
                            price_found = True
                except Exception as ex:
                    logger.info("CSS ì„ íƒìë„ ì‹¤íŒ¨")
            
            if not price_found:
                logger.warning("ëª¨ë“  ê°€ê²© ì¶”ì¶œ ë°©ë²• ì‹¤íŒ¨")
            
            # ì œëª© ì¶”ì¶œ
            try:
                for xpath in self.XPATHS.get('title', []):
                    try:
                        title_element = self.driver.find_element(By.XPATH, xpath)
                        result['title'] = title_element.text.strip()
                        logger.info(f"ì œëª©: {result['title'][:50]}...")
                        break
                    except:
                        continue
            except Exception as e:
                logger.warning(f"ì œëª© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            # ì´ë¯¸ì§€ URL ì¶”ì¶œ
            try:
                for xpath in self.XPATHS.get('imageurl', []):
                    try:
                        image_element = self.driver.find_element(By.XPATH, xpath)
                        result['imageurl'] = image_element.get_attribute('src')
                        logger.info(f"ì´ë¯¸ì§€ URL: {result['imageurl'][:50]}...")
                        break
                    except:
                        continue
                
                # ê°€ê²©ì´ ì—†ê³  ì´ë¯¸ì§€ë„ ì¶”ì¶œë˜ì§€ ì•Šì€ ê²½ìš° fallback ì„ íƒì ì‹œë„
                if result['retailprice'] is None and result['imageurl'] is None:
                    logger.info("ğŸ’¡ ê°€ê²©ì´ ì—†ì–´ fallback ì´ë¯¸ì§€ ì„ íƒìë¡œ ì‹œë„ ì¤‘...")
                    for xpath in self.XPATHS.get('imageurl_fallback', []):
                        try:
                            image_element = self.driver.find_element(By.XPATH, xpath)
                            result['imageurl'] = image_element.get_attribute('src')
                            logger.info(f"âœ… Fallback ì´ë¯¸ì§€ URL: {result['imageurl'][:50]}...")
                            break
                        except Exception as xe:
                            logger.info(f"âŒ Fallback ì„ íƒì ì‹¤í–‰ ì‹¤íŒ¨: {xe}")
                            continue
            except Exception as e:
                logger.warning(f"ì´ë¯¸ì§€ URL ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ í˜ì´ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            
            # ê¸°ë³¸ê°’ ë°˜í™˜
            now_time = datetime.now(self.korea_tz)
            crawl_datetime_str = now_time.strftime('%Y-%m-%d %H:%M:%S')
            crawl_strdatetime = now_time.strftime('%Y%m%d%H%M%S') + f"{now_time.microsecond:06d}"[:4]
            
            return {
                'retailerid': row_data.get('retailerid', ''),
                'country_code': row_data.get('country', 'usa'),
                'ships_from': 'usa',
                'channel': row_data.get('channel', 'Online'),
                'retailersku': row_data.get('retailersku', ''),
                'brand': row_data.get('brand', ''),
                'brand_eng': row_data.get('brand_eng', row_data.get('brand', '')),
                'form_factor': row_data.get('form_factor', ''),
                'segment_lv1': row_data.get('seg_lv1', ''),
                'segment_lv2': row_data.get('seg_lv2', ''),
                'segment_lv3': row_data.get('seg_lv3', ''),
                'capacity': row_data.get('capacity', ''),
                'item': row_data.get('item', ''),
                'retailprice': None,
                'sold_by': 'BestBuy',
                'imageurl': None,
                'producturl': url,
                'crawl_datetime': crawl_datetime_str,
                'crawl_strdatetime': crawl_strdatetime,
                'title': None,
                'vat': row_data.get('vat', 'x')
            }

    def extract_with_retry(self, url, row_data, max_retries=2):
        """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ ì¶”ì¶œ"""
        for attempt in range(max_retries + 1):
            try:
                if attempt > 0:
                    logger.info(f"ğŸ”„ ì¬ì‹œë„ {attempt}/{max_retries}")
                    time.sleep(random.uniform(5, 10))  # ì¬ì‹œë„ ì „ ëŒ€ê¸°
                
                result = self.extract_product_info(url, row_data)
                
                # ì„±ê³µ ì¡°ê±´: ê°€ê²©ì´ ì¶”ì¶œë˜ì—ˆê±°ë‚˜ ì œëª©ì´ë¼ë„ ì¶”ì¶œë¨
                if result['retailprice'] is not None or result['title']:
                    return result
                    
            except Exception as e:
                logger.warning(f"ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}")
                if attempt == max_retries:
                    break
        
        # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
        now_time = datetime.now(self.korea_tz)
        crawl_datetime_str = now_time.strftime('%Y-%m-%d %H:%M:%S')
        crawl_strdatetime = now_time.strftime('%Y%m%d%H%M%S') + f"{now_time.microsecond:06d}"[:4]
        
        return {
            'retailerid': row_data.get('retailerid', ''),
            'country_code': row_data.get('country', 'usa'),
            'ships_from': 'usa',
            'channel': row_data.get('channel', 'Online'),
            'retailersku': row_data.get('retailersku', ''),
            'brand': row_data.get('brand', ''),
            'brand_eng': row_data.get('brand_eng', row_data.get('brand', '')),
            'form_factor': row_data.get('form_factor', ''),
            'segment_lv1': row_data.get('seg_lv1', ''),
            'segment_lv2': row_data.get('seg_lv2', ''),
            'segment_lv3': row_data.get('seg_lv3', ''),
            'capacity': row_data.get('capacity', ''),
            'item': row_data.get('item', ''),
            'retailprice': None,
            'sold_by': 'BestBuy',
            'imageurl': None,
            'producturl': url,
            'crawl_datetime': crawl_datetime_str,
            'crawl_strdatetime': crawl_strdatetime,
            'title': None,
            'vat': row_data.get('vat', 'x')
        }
    
    def save_to_db(self, df):
        """DBì— ê²°ê³¼ ì €ì¥"""
        if self.db_engine is None:
            logger.warning("âš ï¸ DB ì—°ê²°ì´ ì—†ì–´ DB ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤")
            return False
        
        try:
            # bestbuy_price_crawl_tbl_usa í…Œì´ë¸”ì— ì €ì¥
            df.to_sql('bestbuy_price_crawl_tbl_usa', self.db_engine, if_exists='append', index=False)
            logger.info(f"âœ… DB ì €ì¥ ì™„ë£Œ: {len(df)}ê°œ ë ˆì½”ë“œ")
            
            # í¬ë¡¤ë§ ë¡œê·¸ë¥¼ pandas DataFrameìœ¼ë¡œ ë§Œë“¤ì–´ì„œ í•œë²ˆì— ì €ì¥
            log_records = []
            for _, row in df.iterrows():
                log_records.append({
                    'country_code': 'usa',
                    'url': row['producturl'],
                    'error_message': None if row['retailprice'] is not None else 'Price not found',
                    'execution_time': random.uniform(3, 10),
                    'retailprice': row['retailprice'],
                    'crawl_datetime': row['crawl_datetime']
                })
            
            if log_records:
                log_df = pd.DataFrame(log_records)
                log_df.to_sql('amazon_crawl_logs', self.db_engine, if_exists='append', index=False)
                logger.info(f"âœ… í¬ë¡¤ë§ ë¡œê·¸ ì €ì¥ ì™„ë£Œ: {len(log_records)}ê°œ")
            
            # ì €ì¥ëœ ë°ì´í„° í™•ì¸
            with self.db_engine.connect() as conn:
                count_query = "SELECT COUNT(*) FROM bestbuy_price_crawl_tbl_usa WHERE DATE(crawl_datetime) = CURDATE()"
                result = conn.execute(count_query)
                today_count = result.scalar()
                logger.info(f"ğŸ“Š ì˜¤ëŠ˜ ì €ì¥ëœ ì´ ë ˆì½”ë“œ: {today_count}ê°œ")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False
    
    def upload_to_file_server(self, local_file_path, remote_filename=None, country_code='usa'):
        """íŒŒì¼ì„œë²„ì— ì—…ë¡œë“œ"""
        try:
            # SFTP ì—°ê²°
            transport = paramiko.Transport((FILE_SERVER_CONFIG['host'], FILE_SERVER_CONFIG['port']))
            transport.connect(
                username=FILE_SERVER_CONFIG['username'],
                password=FILE_SERVER_CONFIG['password']
            )
            sftp = paramiko.SFTPClient.from_transport(transport)
            
            # ì›ê²© íŒŒì¼ëª… ì„¤ì •
            if remote_filename is None:
                remote_filename = os.path.basename(local_file_path)
            
            # êµ­ê°€ë³„ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            country_dir = f"{FILE_SERVER_CONFIG['upload_path']}/{country_code}"
            
            # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
            try:
                sftp.stat(country_dir)
            except FileNotFoundError:
                logger.info(f"ğŸ“ ë””ë ‰í† ë¦¬ ìƒì„±: {country_dir}")
                sftp.mkdir(country_dir)
            
            # ì—…ë¡œë“œ ê²½ë¡œ
            remote_path = f"{country_dir}/{remote_filename}"
            
            # íŒŒì¼ ì—…ë¡œë“œ
            sftp.put(local_file_path, remote_path)
            logger.info(f"âœ… íŒŒì¼ì„œë²„ ì—…ë¡œë“œ ì™„ë£Œ: {remote_path}")
            
            # ì—°ê²° ì¢…ë£Œ
            sftp.close()
            transport.close()
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ì„œë²„ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def save_results(self, df, save_db=True, upload_server=True):
        """ê²°ê³¼ë¥¼ DBì™€ íŒŒì¼ì„œë²„ì— ì €ì¥"""
        # ìƒˆë¡œìš´ íŒŒì¼ëª… í˜•ì‹: {ìˆ˜ì§‘ì¼ì}{ìˆ˜ì§‘ì‹œê°„}_{êµ­ê°€ì½”ë“œ}_{ì‡¼í•‘ëª°}.csv
        now = datetime.now(self.korea_tz)
        date_str = now.strftime("%Y%m%d")
        time_str = now.strftime("%H%M%S")
        country_code = "usa"
        mall_name = "bestbuy"
        
        # íŒŒì¼ëª… ìƒì„±
        base_filename = f"{date_str}{time_str}_{country_code}_{mall_name}"
        
        results = {
            'db_saved': False,
            'server_uploaded': False
        }
        
        # DB ì €ì¥
        if save_db:
            results['db_saved'] = self.save_to_db(df)
        
        # íŒŒì¼ì„œë²„ ì—…ë¡œë“œ
        if upload_server:
            try:
                # CSV ì„ì‹œ íŒŒì¼
                temp_csv = f'temp_{base_filename}.csv'
                df.to_csv(temp_csv, index=False, encoding='utf-8-sig')
                
                # íŒŒì¼ì„œë²„ ì—…ë¡œë“œ
                remote_csv_filename = f'{base_filename}.csv'
                if self.upload_to_file_server(temp_csv, remote_csv_filename, country_code):
                    results['server_uploaded'] = True
                
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                os.remove(temp_csv)
                logger.info("ğŸ—‘ï¸ ì„ì‹œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ")
                
                # ì—…ë¡œë“œëœ íŒŒì¼ëª… ë¡œê·¸
                logger.info(f"ğŸ“ ì—…ë¡œë“œëœ íŒŒì¼:")
                logger.info(f"   - CSV: {remote_csv_filename}")
                
            except Exception as e:
                logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        
        return results
    
    def test_connection(self):
        """ì—°ê²° í…ŒìŠ¤íŠ¸ ë° ì„¸ì…˜ ì´ˆê¸°í™”"""
        logger.info("=== BestBuy ì„¸ì…˜ ì´ˆê¸°í™” ë° í…ŒìŠ¤íŠ¸ ===")
        
        if not self.setup_driver():
            return False
        
        try:
            # 1ë‹¨ê³„: Google ì—°ê²° í…ŒìŠ¤íŠ¸
            logger.info("1ë‹¨ê³„: Google ì—°ê²° í…ŒìŠ¤íŠ¸...")
            self.driver.get("https://www.google.com")
            time.sleep(2)
            google_title = self.driver.title
            
            if "Google" in google_title:
                logger.info("âœ… Google ì ‘ì† ì„±ê³µ")
            else:
                logger.warning("âš ï¸ Google ì ‘ì† ì´ìƒ")
            
            # 2ë‹¨ê³„: BestBuy ì„¸ì…˜ ì´ˆê¸°í™”
            logger.info("2ë‹¨ê³„: BestBuy ì„¸ì…˜ ì´ˆê¸°í™”...")
            if not self.initialize_session():
                return False
            
            # 3ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ìƒí’ˆ í˜ì´ì§€ ì ‘ì†
            logger.info("3ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ìƒí’ˆ í˜ì´ì§€ ì ‘ì†...")
            test_url = "https://www.bestbuy.com/site/samsung-9100-pro-1tb-internal-ssd-pcie-gen-5x4-nvme-speeds-up-to-14700-mb-s/6618929.p?skuId=6618929"
            
            test_row = {
                'url': test_url,
                'brand': 'Samsung',
                'item': 'Test Item',
                'country': 'usa'
            }
            
            test_result = self.extract_product_info(test_url, test_row)
            
            logger.info("ì¶”ì¶œëœ ì •ë³´:")
            logger.info(f"  - ìƒí’ˆëª…: {test_result['title'][:50] if test_result['title'] else 'None'}...")
            logger.info(f"  - ê°€ê²©: {test_result['retailprice']}")
            logger.info(f"  - ì´ë¯¸ì§€: {'ì¶”ì¶œë¨' if test_result['imageurl'] else 'ì—†ìŒ'}")
            
            # 4ë‹¨ê³„: íŒŒì¼ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸
            logger.info("4ë‹¨ê³„: íŒŒì¼ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸...")
            try:
                transport = paramiko.Transport((FILE_SERVER_CONFIG['host'], FILE_SERVER_CONFIG['port']))
                transport.connect(
                    username=FILE_SERVER_CONFIG['username'],
                    password=FILE_SERVER_CONFIG['password']
                )
                transport.close()
                logger.info("âœ… íŒŒì¼ì„œë²„ ì—°ê²° ì„±ê³µ")
            except Exception as e:
                logger.warning(f"âš ï¸ íŒŒì¼ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
            
            if test_result['retailprice'] is not None or test_result['title']:
                logger.info("âœ… í…ŒìŠ¤íŠ¸ ì„±ê³µ - í¬ë¡¤ë§ ì¤€ë¹„ ì™„ë£Œ!")
                return True
            else:
                logger.warning("âš ï¸ í…ŒìŠ¤íŠ¸ ë¶€ë¶„ ì‹¤íŒ¨ - ê·¸ë˜ë„ ê³„ì† ì§„í–‰")
                return True
                
        except Exception as e:
            logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return False
    
    def scrape_urls(self, urls_data, max_items=None):
        """ì—¬ëŸ¬ URL ìŠ¤í¬ë˜í•‘"""
        if max_items:
            urls_data = urls_data[:max_items]
        
        logger.info(f"ğŸ“Š ì´ {len(urls_data)}ê°œ ì œí’ˆ ì²˜ë¦¬ ì‹œì‘")
        
        results = []
        failed_urls = []
        
        try:
            for idx, row in enumerate(urls_data):
                logger.info(f"\n{'='*50}")
                logger.info(f"ì§„í–‰ë¥ : {idx + 1}/{len(urls_data)} ({(idx + 1)/len(urls_data)*100:.1f}%)")
                
                # URL ì¶”ì¶œ
                url = row.get('url')
                
                # ì œí’ˆ ì •ë³´ ì¶”ì¶œ (ì¬ì‹œë„ ë¡œì§ ì ìš©)
                result = self.extract_with_retry(url, row)
                
                # ì‹¤íŒ¨ ì—¬ë¶€ í™•ì¸
                if result['retailprice'] is None:
                    failed_urls.append({
                        'url': url,
                        'item': row.get('item', ''),
                        'brand': row.get('brand', '')
                    })
                
                results.append(result)
                
                # 10ê°œë§ˆë‹¤ DBì— ì¤‘ê°„ ì €ì¥
                if (idx + 1) % 10 == 0:
                    interim_df = pd.DataFrame(results[-10:])
                    if self.db_engine:
                        try:
                            interim_df.to_sql('bestbuy_price_crawl_tbl_usa', self.db_engine, 
                                            if_exists='append', index=False)
                            logger.info(f"ğŸ’¾ ì¤‘ê°„ ì €ì¥: 10ê°œ ë ˆì½”ë“œ DB ì €ì¥")
                        except Exception as e:
                            logger.error(f"ì¤‘ê°„ ì €ì¥ ì‹¤íŒ¨: {e}")
                
                # ë‹¤ìŒ ìš”ì²­ ì „ ëŒ€ê¸°
                if idx < len(urls_data) - 1:
                    wait_time = random.uniform(2, 5)  # BestBuyëŠ” ì¡°ê¸ˆ ë” ë¹ ë¥´ê²Œ
                    logger.info(f"â³ {wait_time:.1f}ì´ˆ ëŒ€ê¸° ì¤‘...")
                    time.sleep(wait_time)
                    
                    # 10ê°œë§ˆë‹¤ ê¸´ íœ´ì‹
                    if (idx + 1) % 10 == 0:
                        logger.info("â˜• 10ê°œ ì²˜ë¦¬ ì™„ë£Œ, 20ì´ˆ íœ´ì‹...")
                        time.sleep(20)
        
        except Exception as e:
            logger.error(f"âŒ ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜: {e}")
        
        finally:
            # ì‹¤íŒ¨ URL ë¡œê·¸
            if failed_urls:
                logger.warning(f"\nâš ï¸ ê°€ê²© ì¶”ì¶œ ì‹¤íŒ¨í•œ URL {len(failed_urls)}ê°œ:")
                for fail in failed_urls[:5]:
                    logger.warning(f"  - {fail['brand']} {fail['item']}: {fail['url']}")
                if len(failed_urls) > 5:
                    logger.warning(f"  ... ì™¸ {len(failed_urls) - 5}ê°œ")
        
        return pd.DataFrame(results)
    
    def analyze_results(self, df):
        """ê²°ê³¼ ë¶„ì„"""
        logger.info("\nğŸ“Š === ê²°ê³¼ ë¶„ì„ ===")
        
        total = len(df)
        with_price = df['retailprice'].notna().sum()
        without_price = df['retailprice'].isna().sum()
        success_rate = (with_price / total * 100) if total > 0 else 0
        
        logger.info(f"ì „ì²´ ì œí’ˆ: {total}ê°œ")
        logger.info(f"ê°€ê²© ì¶”ì¶œ ì„±ê³µ: {with_price}ê°œ")
        logger.info(f"ê°€ê²© ì¶”ì¶œ ì‹¤íŒ¨: {without_price}ê°œ")
        logger.info(f"ì„±ê³µë¥ : {success_rate:.1f}%")
        
        if with_price > 0:
            price_df = df[df['retailprice'].notna()].copy()
            price_df['numeric_price'] = pd.to_numeric(price_df['retailprice'], errors='coerce')
            
            logger.info(f"\nğŸ’° ê°€ê²© í†µê³„:")
            logger.info(f"í‰ê· ê°€: ${price_df['numeric_price'].mean():.2f}")
            logger.info(f"ìµœì €ê°€: ${price_df['numeric_price'].min():.2f}")
            logger.info(f"ìµœê³ ê°€: ${price_df['numeric_price'].max():.2f}")
            logger.info(f"ì¤‘ê°„ê°’: ${price_df['numeric_price'].median():.2f}")
            
            # ë¸Œëœë“œë³„ í†µê³„
            if 'brand' in df.columns:
                brand_stats = price_df['brand'].value_counts()
                logger.info(f"\nğŸ“ˆ ë¸Œëœë“œë³„ ì„±ê³µ:")
                for brand, count in brand_stats.items():
                    logger.info(f"  {brand}: {count}ê°œ")
            
            # ìš©ëŸ‰ë³„ í‰ê·  ê°€ê²©
            if 'capacity' in df.columns:
                capacity_stats = price_df.groupby('capacity')['numeric_price'].agg(['mean', 'count'])
                logger.info(f"\nğŸ’¾ ìš©ëŸ‰ë³„ í‰ê·  ê°€ê²©:")
                for capacity, stats in capacity_stats.iterrows():
                    logger.info(f"  {capacity}: ${stats['mean']:.2f} ({int(stats['count'])}ê°œ)")

def get_db_history(engine, days=7):
    """DBì—ì„œ ìµœê·¼ ê¸°ë¡ ì¡°íšŒ"""
    try:
        query = f"""
        SELECT DATE(crawl_datetime) as date, 
               COUNT(*) as total_count,
               SUM(CASE WHEN retailprice IS NOT NULL THEN 1 ELSE 0 END) as with_price,
               COUNT(DISTINCT brand) as brands,
               COUNT(DISTINCT item) as items
        FROM bestbuy_price_crawl_tbl_usa
        WHERE crawl_datetime >= DATE_SUB(NOW(), INTERVAL {days} DAY)
        GROUP BY DATE(crawl_datetime)
        ORDER BY date DESC
        """
        
        df = pd.read_sql(query, engine)
        logger.info(f"\nğŸ“… ìµœê·¼ {days}ì¼ í¬ë¡¤ë§ ê¸°ë¡:")
        if not df.empty:
            print(df.to_string(index=False))
        else:
            logger.info("ìµœê·¼ í¬ë¡¤ë§ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        logger.error(f"DB ì¡°íšŒ ì˜¤ë¥˜: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\nğŸš€ BestBuy ê°€ê²© ì¶”ì¶œ ì‹œìŠ¤í…œ - DB ê¸°ë°˜ ë²„ì „")
    print("="*60)
    
    # ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”
    scraper = BestBuyScraper()
    
    if scraper.db_engine is None:
        logger.error("DB ì—°ê²° ì‹¤íŒ¨ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    # ìµœê·¼ í¬ë¡¤ë§ ê¸°ë¡ í™•ì¸
    get_db_history(scraper.db_engine, 7)
    
    # ì—°ê²° í…ŒìŠ¤íŠ¸ ë° ì„¸ì…˜ ì´ˆê¸°í™”
    if not scraper.test_connection():
        logger.error("ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        if scraper.driver:
            scraper.driver.quit()
        return
    
    try:
        # ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰
        logger.info("\nğŸ“Š ì „ì²´ í¬ë¡¤ë§ ì‹œì‘")
        urls_data = scraper.get_crawl_targets()
        
        if not urls_data:
            logger.warning("í¬ë¡¤ë§ ëŒ€ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        logger.info(f"âœ… í¬ë¡¤ë§ ëŒ€ìƒ: {len(urls_data)}ê°œ")
        
        # í¬ë¡¤ë§ ì‹¤í–‰
        results_df = scraper.scrape_urls(urls_data)
        
        if results_df is None or results_df.empty:
            logger.error("í¬ë¡¤ë§ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ê²°ê³¼ ë¶„ì„
        failed_count = results_df['retailprice'].isna().sum()
        success_count = results_df['retailprice'].notna().sum()
        success_rate = (success_count / len(results_df) * 100) if len(results_df) > 0 else 0
        
        logger.info(f"\nğŸ“Š === ìµœì¢… ê²°ê³¼ ===")
        logger.info(f"ì „ì²´: {len(results_df)}ê°œ")
        logger.info(f"ì„±ê³µ: {success_count}ê°œ")
        logger.info(f"ì‹¤íŒ¨: {failed_count}ê°œ")
        logger.info(f"ì„±ê³µë¥ : {success_rate:.1f}%")
        
        # DBì™€ íŒŒì¼ì„œë²„ì— ê²°ê³¼ ì €ì¥
        save_results = scraper.save_results(
            results_df,
            save_db=True,
            upload_server=True
        )
        
        # ìƒì„¸ ë¶„ì„
        scraper.analyze_results(results_df)
        
        # ì €ì¥ ê²°ê³¼ ì¶œë ¥
        logger.info("\nğŸ“Š ì €ì¥ ê²°ê³¼:")
        logger.info(f"DB ì €ì¥: {'âœ… ì„±ê³µ' if save_results['db_saved'] else 'âŒ ì‹¤íŒ¨'}")
        logger.info(f"íŒŒì¼ì„œë²„ ì—…ë¡œë“œ: {'âœ… ì„±ê³µ' if save_results['server_uploaded'] else 'âŒ ì‹¤íŒ¨'}")
        
        # ì‹¤íŒ¨í•œ URL ë¡œê·¸
        if failed_count > 0:
            logger.warning(f"\nâš ï¸ {failed_count}ê°œ URLì—ì„œ ê°€ê²© ì¶”ì¶œ ì‹¤íŒ¨")
            failed_items = results_df[results_df['retailprice'].isna()]
            logger.warning("ì‹¤íŒ¨ ëª©ë¡ (ìƒìœ„ 5ê°œ):")
            for idx, row in failed_items.head().iterrows():
                logger.warning(f"  - {row['brand']} {row['item']}: {row['producturl'][:50]}...")
        
        logger.info("\nâœ… í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")
        
    finally:
        # ë“œë¼ì´ë²„ ì¢…ë£Œ
        if scraper.driver:
            scraper.driver.quit()
            logger.info("ğŸ”§ ë“œë¼ì´ë²„ ì¢…ë£Œ")

if __name__ == "__main__":
    # í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸
    required_packages = [
        'undetected-chromedriver',
        'selenium',
        'pandas',
        'pymysql',
        'sqlalchemy',
        'paramiko',
        'openpyxl'
    ]
    
    print("ğŸ“¦ í•„ìš”í•œ íŒ¨í‚¤ì§€:")
    print("pip install " + " ".join(required_packages))
    print("\nâš ï¸ DB ì„¤ì •ì„ ë¨¼ì € í™•ì¸í•˜ì„¸ìš”:")
    print("DB_CONFIG ë”•ì…”ë„ˆë¦¬ì˜ user, password, host ì •ë³´ë¥¼ ì‹¤ì œ ê°’ìœ¼ë¡œ ë³€ê²½í•´ì•¼ í•©ë‹ˆë‹¤.")
    print()
    
    main()